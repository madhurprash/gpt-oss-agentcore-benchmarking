{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8adfd138",
   "metadata": {},
   "source": [
    "# Fine-tune `GPT-OSS 20B` on `Unsloth` using Supervised Fine-Tuning (SFT)\n",
    "---\n",
    "\n",
    "We will use unsloth to fine tune the gpt oss model. Unsloth is an open source library that accelerates and optimizes the fine-tuning of large language models, by re writing the computations of triton kernels. It achieves this by combining techniques like QLoRA, Flash attention, and 4-bit quantization to reduce the memory usage and increase the training speed by up to 2x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1adede8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth currently only works on NVIDIA GPUs and Intel GPUs.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# import fast language model library from unsloth\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# fast language model is a core component within the Unsloth library, \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# designed to faciliate efficient and fast memory effective fine-tuning and inference of LLMs.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Fast Language Model aims to reduce VRAM consumption during LLM operations, enabling the fine-tuning\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# of larger models on modest hardware.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# This is also designed to be fully compatible with HF libraries like transformers, PEFT, TRL.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gpt-oss-agentcore-benchmarking-2/.venv/lib/python3.12/site-packages/unsloth/__init__.py:79\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth currently only works on NVIDIA GPUs and Intel GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m DEVICE_TYPE : \u001b[38;5;28mstr\u001b[39m = \u001b[43mget_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_count\u001b[39m():\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m DEVICE_TYPE == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gpt-oss-agentcore-benchmarking-2/.venv/lib/python3.12/site-packages/unsloth/__init__.py:77\u001b[39m, in \u001b[36mget_device_type\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m torch.xpu.is_available():\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth currently only works on NVIDIA GPUs and Intel GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import logging\n",
    "# import fast language model library from unsloth\n",
    "# fast language model is a core component within the Unsloth library, \n",
    "# designed to faciliate efficient and fast memory effective fine-tuning and inference of LLMs.\n",
    "# Fast Language Model aims to reduce VRAM consumption during LLM operations, enabling the fine-tuning\n",
    "# of larger models on modest hardware.\n",
    "# This is also designed to be fully compatible with HF libraries like transformers, PEFT, TRL.\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove existing handlers\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Add a simple handler\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14424446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the 4bit pre quantized models that we support for 4x faster downloading + no OOMs\n",
    "# These are all models available on hugging face hub\n",
    "fourbit_modes = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "]\n",
    "\n",
    "# Now, we will load the pre-trained model. A pre-trained model is a model that has been previously\n",
    "# trained on several large datasets with billions of parameters. This model has been trained on a large corpus of text data\n",
    "# and has learned to generate human-like text. We will use this pre-trained model as a starting point for our fine-tuning process.\n",
    "hf_model_name = \"openai/gpt-oss-20b\"\n",
    "logger.info(f\"Going to fine-tune the model {hf_model_name} using Unsloth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Going to load the model {hf_model_name} using unsloth fast language model\")\n",
    "# load the pre-trained model using unsloth fast language model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=hf_model_name,\n",
    "    # This sets the precision of the model weights and computations\n",
    "    dtype=None, \n",
    "    # Maximum context length the model will proces in one forward pass (number of tokens)\n",
    "    max_seq_length=4096, \n",
    "    load_in_4bit=False, # load the model in 4bit mode\n",
    "    full_finetuning=False,\n",
    "    # token = ...\n",
    ")\n",
    "logger.info(f\"Loaded the model {model} using unsloth fast language model\")\n",
    "\n",
    "# Reasoning effort for GPT OSS models\n",
    "\"\"\"\n",
    "The GPT OSS models are designed to handle complex reasoning tasks, including multi-step reasoning at varying levelts, \n",
    "that allows users to adjust the model's reasoning dwelling time based on the complexity of the task at hand. \n",
    "\n",
    "The gpt-oss models offer three distinct levels of reasoning effort you can choose from:\n",
    "\n",
    "- Low: Optimized for tasks that need very fast responses and don't require complex, multi-step reasoning.\n",
    "- Medium: A balance between performance and speed.\n",
    "- High: Provides the strongest reasoning performance for tasks that require it, though this results in higher latency.\n",
    "\"\"\"\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# define the messages to experiment with\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-oss-agentcore-benchmarking-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
