# Model Configuration
model:
  name: "microsoft/DialoGPT-medium"  
  max_length: 512
  cache_dir: "./model_cache"

# Training Configuration
training:
  output_dir: "./results"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 2
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# LoRA Configuration (for efficient fine-tuning)
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# Dataset Configuration
dataset:
  name: "your-dataset-name"
  train_file: "train.json"
  validation_file: "validation.json"
  text_column: "text"
  max_train_samples: null
  max_eval_samples: null

# this contains information about what configuration the 
# model should be hosted on
infra_layer:
  # this is the EC2 instance where the model is trained and deployed
  instance_type: 


# S3 Configuration
s3:
  bucket: 
  input_path: "s3://your-s3-bucket/input/"
  output_path: "s3://your-s3-bucket/output/"
  model_path: "s3://your-s3-bucket/models/"

# Monitoring
monitoring:
  use_wandb: 
  wandb_project: 
  wandb_run_name: 